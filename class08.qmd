---
title: "Class 8 Mini Project"
author: "Garrett Cole"
format: pdf
---

Preparing the data

```{r}
fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names = 1)
View(wisc.df)
```

Omit the first column and move it to a diagnosis vector

```{r}
wisc.data <- wisc.df[,-1]
diagnosis <- as.factor(wisc.df$diagnosis)

```

## Exploratory Data Analysis

```{r}
View(wisc.data)
dim(wisc.data)
```

# Question 1: How many observations are in this dataset?

569 Rows so 569 Observations

# Question 2: How many of the observations have a malignant diagnosis?

```{r}
sum(wisc.df$diagnosis == "M")
```

212 of the observations ahve a malignant diagnosis

# Question 3: How many variables/features in the data set are suffixed with \_mean

```{r}
length(grep("_mean", colnames(wisc.data)))
```

10 variables/features in the data set are suffixed with \_mean

## Principal Component Analysis

Performing PCA

```{r}
#Check column means and standard deviation
colMeans(wisc.data)

apply(wisc.data, 2, sd)

#Perform PCA on wisc.data
wisc.pr <- prcomp( wisc.data, scale = TRUE )
```

```{r}
#Look at summary of results
summary(wisc.pr)
plot(wisc.pr)
```

# Question 4: From your results, what proportion of the original variance captured by the first principal components (PC1)?

From my results, the proportion of the original variance by the PC1 is 0.4427

# Question 5: How many principal components (PCs) are required to describe at least 70% of the original variance in this data?

3 -\> PC1, PC2, and PC3

# Question 6: How many principal components (PCs) are required to describe at least 90% of the original variance in this data?

7 -\> PC1, PC2, PC3, PC4, PC5, PC6, PC7

# Question 7: What stands out to you about this plot? Is it easy or difficult to understand? Why?

What stands out to me about this plot is that each point on the plot is labeled by the row name which makes it really hard to distinguish the difference between points as the names just overlap onto each other causing a huge black uneven circle on the plot.

```{r}
biplot(wisc.pr)
```

```{r}
# Scatter plot Observations by components 1 and 2
plot( wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis, 
     xlab = "PC1", ylab = "PC2")
```

```{r}
# Scatter plot Observations by components 1 and 3
plot( wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

# Question 8: Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

The first plot between PC1 & PC2 has a more observant separation while the second plot between PC1 & PC3 has more data points overlapping

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot 2 package
library(ggplot2)

#Make a scatter plot colored by diagnosis
ggplot(df) +
  aes(PC1, PC2, col = diagnosis) +
  geom_point()
```

Variance Explained

```{r}
#Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
#Variance explained by each principal componenet: pve
pve <- pr.var/sum(pr.var)

#Plot variance explained for each principal component
plot(pve, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     ylim = c(0,1), type = "o")
```

```{r}
#Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Percent of Variance Explained",
        names.arg=paste0("PC", 1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100)
```

```{r}
##ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

# Question 10: What is the minimum number of principal components required to explain 80% of the variance of the data?

The minimum number of PC to explain 80% of the variance of the data is 5 (PC1-5)

## Hierarchical Clustering

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

## Combining Methods

Clustering on PCA Results

# Question 15: How well does the newly created model with four clusters separate out the two diagnoses?

It seperates out the two diagnoses fairy well as the newly created model with four clusters is more easily to observe

# Question 16: How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km\$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

In terms of separating the diagnoses, the k-means and hierarchical clustering models I created don't do that well compared to the newest models I've created

# Question 17: Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

The analysis procedure which resulted in the best specificity is the hierarchical clustering model. The one with the best sensitivity is the PCA analysis

# Question 18: Which of these new patients should we prioritize for follow up based on your results?

Patient 2 \`\`\`
